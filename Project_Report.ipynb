{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The firststep in our data analysis was to extract the data from our frimary datasources.  The primary datasources for this project was:\n",
    "    *Kaggle\n",
    "    *OpenWeatherMap.org\n",
    "    \n",
    "We retrieved the Olympic data from Kaggle.  The data was already in CSV format when downloaded, so there was not need to convert the data into any other format.  The OpenWeatherMap data had to be downloaded in JSON format via the OpenWeatherMap hitoric weather API.  Once the JSON was recieved, we had to convert the file into CSV for further processing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic transformations:\n",
    "\n",
    "Cleaning: Mapping NULL to 0 or \"Male\" to \"M\" and \"Female\" to \"F,\" date format consistency, etc.\n",
    "\n",
    "Deduplication: Identifying and removing duplicate records\n",
    "\n",
    "Format revision: Character set conversion, unit of measurement conversion, date/time conversion, etc.\n",
    "\n",
    "Key restructuring: Establishing key relationships across tables\n",
    "\n",
    "\n",
    "Advanced transformations:\n",
    "\n",
    "Derivation Applying business rules to your data that derive new calculated values from existing data – for example, \n",
    "creating a revenue metric that subtracts taxes\n",
    "\n",
    "Filtering: Selecting only certain rows and/or columns\n",
    "\n",
    "Joining: Linking data from multiple sources – for example, adding ad spend data across multiple platforms, such as Google Adwords and Facebook Ads\n",
    "\n",
    "Splitting: Splitting a single column into multiple columns\n",
    "\n",
    "Data validation: Simple or complex data validation – for example, if the first three columns in a row are empty then reject the row from processing\n",
    "\n",
    "Summarization: Values are summarized to obtain total figures which are calculated and stored at multiple levels as business metrics – for example, adding up all purchases a customer has made to build a customer lifetime value (CLV) metric\n",
    "\n",
    "Aggregation: Data elements are aggregated from multiple data sources and databases\n",
    "\n",
    "Integration: Give each unique data element one standard name with one standard definition. Data integration reconciles different data names and values for the same data element."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The load stage of the ETL process depends largely on what you intend to do with the data once it’s loaded into the data warehouse. Uses could include:\n",
    "\n",
    "Layering a business intelligence or analytics tool on top of the warehouse\n",
    "Creating a tool for site search\n",
    "Building a machine learning algorithm to detect fraud\n",
    "Implementing a real-time alerting system\n",
    "Regardless of your end goal, one of the key considerations during the load process is understanding the work you’re requiring of the target environment. Depending on your data volume, structure, target, and load type, you could negatively impact the host system when you load data.\n",
    "\n",
    "For example, loading data into Amazon Redshift is best done infrequently in large batches. If you’re loading data into Redshift, you should avoid small, frequent batches or you’ll have angry analysts beating down your door when they notice that your jobs are consuming all of their cluster resources.\n",
    "\n",
    "Bottom line: The load process needs to be specific to what you’re loading data into. We’re going to move forward with the assumption that you’re loading data into an analytics warehouse.\n",
    "\n",
    "There are two primary methods to load data into a warehouse:\n",
    "\n",
    "Full load: entire data dump that takes place the first time a data source is loaded into the warehouse\n",
    "Incremental load: delta between target and source data is dumped at regular intervals. The last extract date is stored so that only records added after this date are loaded. Incremental loads come in two flavors that vary based on the volume of data you’re loading:\n",
    "Streaming incremental load – better for loading small data volumes\n",
    "Batch incremental load – better for loading large data volumes\n",
    "\n",
    "Full load\tIncremental load\n",
    "Rows sync\tAll rows in source data\tNew and updated records only\n",
    "Time\tMore time\tLess time\n",
    "Difficulty\tLow\tHigh. ETL must be checked for new/updated row. Recovery from an issue is harder\n",
    "\n",
    "The initial full load is relatively straightforward. When you start taking on incremental loads, things get more complex. Here are three of the most common problem areas:\n",
    "\n",
    "Ordering: To handle massive scale with high availability, data pipelines are often distributed systems. This means that arriving data points can take different paths through the system, which means they can be processed in a different order than they were received. If data is being updated or deleted, processing in the wrong order will lead to bad data. Maintaining and auditing ordering is critical for keeping data accurate.\n",
    "Schema evolution: What happens to your existing data when a new property is added, or an existing property is changed? Some of these changes can be destructive or leave data in an inconsistent state. For example, what happens if your data warehouse starts receiving string values for a field that is expected to be an integer datatype?\n",
    "Monitorability: With data coming from a large number of sources, failures are inevitable. How long will it take you to catch them? Failure scenarios include:\n",
    "An API is down for maintenance\n",
    "API credentials expire\n",
    "API calls are returning successfully, but do not contain any data\n",
    "Network congestion prevents communication with an API\n",
    "The pipeline destination (e.g. a data warehouse) is offline\n",
    "Any of these problems will likely result in data that is either incomplete or wrong. Recovering from these issues can be a massive headache."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the final analysis we used MySql. We chose to work with my MySql because it is easy toload the data in this relational database. The information is, after loading easy accesible and convertible and to combine different data sources: in our case the olympic data with the weather data. \n",
    "\n",
    "\n",
    "The expected outcomes for this project if we would go into the next step of analysing the data, would be to find a relation between weather and the results of the olympics, in this case the medal count per team (a team is a country). For example if the temperature was low different countries would benefit and score a high amount of medal than when the temperature was high.\n",
    "\n",
    "\n",
    "We chose to work with average temperature and average precepitation because we do not have the information about the exact time the games took place (although it would be possible to find this information, but this will be very time consuming). \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
